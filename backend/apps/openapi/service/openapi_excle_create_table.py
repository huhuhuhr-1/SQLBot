import asyncio
import json
import re
import traceback
from typing import List, Optional, Tuple, Dict, Any

import pandas as pd
from sqlalchemy import text
from fastapi import HTTPException
from pydantic import BaseModel, Field
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser

from dateutil import parser as dtparser
import datetime as _dt

from apps.openapi.llm.my_llm import LLMManager
from common.utils.utils import SQLBotLogUtil  # ç¡®ä¿å­˜åœ¨


# =========================
# å·¥å…·ï¼šPG ç±»å‹åˆ¤æ–­
# =========================
def _is_timestamp_pgtype(pg_type: str) -> bool:
    t = (pg_type or "").lower().strip()
    return "timestamp" in t


def _is_date_pgtype(pg_type: str) -> bool:
    t = (pg_type or "").lower().strip()
    return t == "date"


def _is_time_pgtype(pg_type: str) -> bool:
    t = (pg_type or "").lower().strip()
    return t == "time"


def _is_integer_pgtype(pg_type: str) -> bool:
    t = (pg_type or "").lower().strip()
    return t in {"smallint", "integer", "bigint", "int2", "int4", "int8"}


def _is_numeric_pgtype(pg_type: str) -> bool:
    t = (pg_type or "").lower().strip()
    return ("numeric" in t) or (t in {"decimal", "real", "double precision", "float", "float4", "float8"})


def _is_boolean_pgtype(pg_type: str) -> bool:
    t = (pg_type or "").lower().strip()
    return t == "boolean"


# =========================
# å·¥å…·ï¼šå€¼çº§åˆ«æ¸…æ´—ä¸è§„èŒƒåŒ–
# =========================
def _normalize_to_str_ts(value) -> Optional[str]:
    """è§„èŒƒä¸º 'YYYY-MM-DD HH:MM:SS' å­—ç¬¦ä¸²ï¼›å°½åŠ›è§£æå„ç§æ ¼å¼"""
    if value is None:
        return None
    s = str(value).strip()
    if s == "" or s.lower() in {"nan", "nat", "none", "null"}:
        return None

    # pandas/datetime ç›´æ¥æ ¼å¼åŒ–
    try:
        if hasattr(value, "to_pydatetime"):
            dt = value.to_pydatetime()
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        if isinstance(value, _dt.datetime):
            return value.strftime("%Y-%m-%d %H:%M:%S")
        if isinstance(value, _dt.date):
            return _dt.datetime(value.year, value.month, value.day).strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        pass

    # Excel åºåˆ—å·ï¼ˆæŒ‰å¤©ï¼‰
    try:
        if isinstance(value, (int, float)) or s.replace(".", "", 1).isdigit():
            num = float(value)
            if 59 <= num <= 100000:
                dt = pd.to_datetime(num, unit="D", origin="1899-12-30", errors="raise")
                return dt.to_pydatetime().strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        pass

    # åªæœ‰â€œå¹´-æœˆ/å¹´.æœˆ/å¹´/æœˆâ€çš„æƒ…å†µé»˜è®¤è¡¥ 01 æ—¥
    try:
        ym_like = False
        if re.fullmatch(r"\d{4}[-/.]\d{1,2}", s) or ("å¹´" in s and "æ—¥" not in s and "å·" not in s):
            ym_like = True
        if ym_like:
            dt = dtparser.parse(s, default=_dt.datetime(1970, 1, 1))
            return dt.strftime("%Y-%m-%d %H:%M:%S")

        dt = dtparser.parse(s, default=_dt.datetime(1970, 1, 1))
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        # è¿”å›åŸå€¼å­—ç¬¦ä¸²ï¼Œè®©åç»­é™çº§ç­–ç•¥æ¥ç®¡
        return s


def _normalize_to_str_date(value) -> Optional[str]:
    """è§„èŒƒä¸º 'YYYY-MM-DD' å­—ç¬¦ä¸²ï¼›å°½åŠ›è§£æå„ç§æ ¼å¼"""
    if value is None:
        return None
    s = str(value).strip()
    if s == "" or s.lower() in {"nan", "nat", "none", "null"}:
        return None
    try:
        if hasattr(value, "to_pydatetime"):
            d = value.to_pydatetime().date()
            return d.strftime("%Y-%m-%d")
        if isinstance(value, _dt.datetime):
            return value.date().strftime("%Y-%m-%d")
        if isinstance(value, _dt.date):
            return value.strftime("%Y-%m-%d")
    except Exception:
        pass
    try:
        # Excel åºåˆ—å·
        if isinstance(value, (int, float)) or s.replace(".", "", 1).isdigit():
            num = float(value)
            if 59 <= num <= 100000:
                dt = pd.to_datetime(num, unit="D", origin="1899-12-30", errors="raise")
                return dt.date().strftime("%Y-%m-%d")
    except Exception:
        pass
    try:
        d = dtparser.parse(s, default=_dt.datetime(1970, 1, 1)).date()
        return d.strftime("%Y-%m-%d")
    except Exception:
        return s


def _coerce_boolean(v) -> Optional[bool]:
    if v is None:
        return None
    s = str(v).strip().lower()
    if s in {"true", "t", "1", "y", "yes", "æ˜¯", "å¯¹", "çœŸ"}:
        return True
    if s in {"false", "f", "0", "n", "no", "å¦", "é”™", "å‡"}:
        return False
    return None  # äº¤ç”±é™çº§ç­–ç•¥


def _coerce_integer(v) -> Optional[int]:
    if v is None or str(v).strip() == "":
        return None
    s = str(v).strip().replace(",", "")  # å»åƒåˆ†ä½
    if s.endswith("%"):
        return None
    # å…ˆå°è¯•æµ®è½¬æ•´ï¼Œå…¼å®¹ "12.0"
    try:
        f = float(s)
        if abs(f - round(f)) < 1e-9:
            return int(round(f))
    except Exception:
        pass
    try:
        return int(s)
    except Exception:
        return None


def _coerce_numeric(v) -> Optional[float]:
    if v is None or str(v).strip() == "":
        return None
    s = str(v).strip().replace(",", "")  # å»åƒåˆ†ä½
    try:
        if s.endswith("%"):
            s = s[:-1].strip()
            return float(s) / 100.0
        # ä¼šè®¡è´Ÿå· (123) â†’ -123
        if re.fullmatch(r"\(\s*\d+(\.\d+)?\s*\)", s):
            s = "-" + s.strip("()").strip()
        return float(s)
    except Exception:
        return None


# =========================
# Pydanticï¼šLLM è¾“å‡ºç»“æ„
# =========================
class FieldInfo(BaseModel):
    column: str = Field(..., description="è‹±æ–‡å­—æ®µåï¼ˆå°å†™ä¸‹åˆ’çº¿ï¼‰")
    name_cn: str = Field(..., description="åŸå§‹ä¸­æ–‡åˆ—å")
    type: str = Field(..., description="PG ç±»å‹ï¼Œå¦‚ varchar(2048)/integer/numeric(10,2)/timestamp/date")
    comment: str = Field(..., description="ç®€æ´æ³¨é‡Š")


class TableSchema(BaseModel):
    fields: List[FieldInfo]
    table_comment: Optional[str] = Field(default="", description="è¡¨çº§æ³¨é‡Š")


# =========================
# æ ‡è¯†ç¬¦ & ç±»å‹ å®‰å…¨æ¶ˆæ¯’
# =========================
_PG_IDENT_MAX = 63
_IDENT_RE = re.compile(r"^[a-z_][a-z0-9_]*$")


def _sanitize_identifier(raw: str) -> str:
    """æŠŠä»»æ„å­—ç¬¦ä¸²è½¬ä¸ºå®‰å…¨çš„ PG æ ‡è¯†ç¬¦ï¼ˆå°å†™ä¸‹åˆ’çº¿ï¼Œä»… ASCIIï¼‰ï¼Œé™åˆ¶ 63 å­—èŠ‚ã€‚"""
    if raw is None:
        raw = "col"
    name = re.sub(r"[^a-zA-Z0-9_]+", "_", raw.strip().lower())
    if not name or name[0].isdigit():
        name = f"col_{name}"
    if len(name) > _PG_IDENT_MAX:
        name = name[:_PG_IDENT_MAX]
    if not _IDENT_RE.match(name):
        name = "col_" + re.sub(r"[^a-z0-9_]", "", name)
        name = name[:_PG_IDENT_MAX]
    return name


def _ensure_unique_columns(cols: List[str]) -> List[str]:
    """sanitize + å»é‡ + 63å­—èŠ‚å†²çªè§„é¿"""
    seen = {}
    out = []
    for c in cols:
        base = _sanitize_identifier(c or "col")
        name = base
        k = 1
        while name in seen:
            suffix = f"_{k}"
            name = (base[: max(1, _PG_IDENT_MAX - len(suffix))]) + suffix
            k += 1
        seen[name] = True
        out.append(name)
    return out


_TYPE_VARCHAR_RE = re.compile(r"^varchar\((\d+)\)$", re.I)
_TYPE_NUMERIC_RE = re.compile(r"^numeric\((\d+)\s*,\s*(\d+)\)$", re.I)


def _sanitize_pg_type(pg_type: str, series: Optional[pd.Series] = None) -> str:
    """
    ç±»å‹ç™½åå•ä¸é™å¹…ï¼š
    - numeric(p,s)ï¼šp<=38, s<=10ï¼Œè¶…å‡ºé™å¹…
    - integer/bigint/smallint/boolean/timestamp/date/time/text ä¿ç•™
    - å…¶å®ƒ/æœªçŸ¥ç±»å‹ â†’ text
    """
    if not pg_type:
        return "text"
    t = pg_type.strip().lower().replace(" ", "")

    if t in {
        "text", "boolean", "smallint", "integer", "bigint", "timestamp", "date", "time",
        "int2", "int4", "int8", "real", "doubleprecision", "float4", "float8"
    }:
        return "timestamp" if t == "timestamp" else ("date" if t == "date" else ("time" if t == "time" else t))

    m = _TYPE_NUMERIC_RE.match(t)
    if m:
        p = min(int(m.group(1)), 38)
        s = min(int(m.group(2)), 10)
        if s > p:
            s = max(0, min(4, p))  # ç®€å•ä¿®æ­£
        return f"numeric({p},{s})"

    if t.startswith("numeric") or t.startswith("decimal"):
        return "numeric(18,4)"

    return "text"


# =========================
# SQL å­—ç¬¦ä¸²å­—é¢é‡è½¬ä¹‰ï¼ˆé¿å…åœ¨ f-string è¡¨è¾¾å¼é‡Œå†™å¸¦å¼•å·çš„å­—é¢é‡ï¼‰
# =========================
def _sql_literal(val: Optional[str]) -> str:
    """æŠŠ Python å­—ç¬¦ä¸²è½¬ä¸º SQL å•å¼•å·å­—é¢é‡å†…å®¹ï¼ˆåªåš ' â†’ '' è½¬ä¹‰ï¼Œä¸åŒ…è£¹å¼•å·ï¼‰"""
    return (val or "").replace("'", "''")


# =========================
# Schema æ ¡éªŒä¸è‡ªä¿ç­–ç•¥
# =========================
def _adjust_text_type_by_length(series: pd.Series, base_type: str = "varchar(2048)", max_varchar: int = 1024) -> str:
    """åŸºäºå®é™…æ•°æ®é•¿åº¦ï¼ŒåŠ¨æ€å†³å®š varchar(N) æˆ– textã€‚"""
    try:
        lengths = series.fillna("").astype(str).map(len)
        max_len = int(lengths.max()) if not lengths.empty else 0
        if max_len == 0:
            return base_type
        n = min(max(max_len, 50), max_varchar)
        if n >= max_varchar:
            return "text"
        return f"varchar({n})"
    except Exception:
        return base_type


def _coerce_series_by_type(series: pd.Series, pg_type: str) -> Tuple[pd.Series, bool]:
    """æŒ‰ pg_type å°è¯•æ•´ä½“åˆ—æ¸…æ´—ã€‚è¿”å›ï¼š(æ¸…æ´—åçš„åˆ—, æ˜¯å¦å®Œå…¨æˆåŠŸ)"""
    t = (pg_type or "").lower().strip()

    if _is_date_pgtype(t):
        cleaned = series.apply(_normalize_to_str_date)
        ok_mask = cleaned.dropna().map(lambda s: bool(re.fullmatch(r"\d{4}-\d{2}-\d{2}", str(s))))
        all_ok = ok_mask.all() if not ok_mask.empty else True
        return cleaned, all_ok

    if _is_timestamp_pgtype(t):
        cleaned = series.apply(_normalize_to_str_ts)
        ok_mask = cleaned.dropna().map(lambda s: bool(re.fullmatch(r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}", str(s))))
        all_ok = ok_mask.all() if not ok_mask.empty else True
        return cleaned, all_ok

    if _is_time_pgtype(t):
        def _norm_time(v):
            if v is None:
                return None
            s = str(v).strip()
            if s == "" or s.lower() in {"nan", "nat", "none", "null"}:
                return None
            try:
                return dtparser.parse(s).time().strftime("%H:%M:%S")
            except Exception:
                return s

        cleaned = series.apply(_norm_time)
        ok_mask = cleaned.dropna().map(lambda s: bool(re.fullmatch(r"\d{2}:\d{2}:\d{2}", str(s))))
        all_ok = ok_mask.all() if not ok_mask.empty else True
        return cleaned, all_ok

    if _is_boolean_pgtype(t):
        cleaned = series.apply(_coerce_boolean)
        all_ok = cleaned.isna().sum() == series.isna().sum()
        return cleaned, all_ok

    if _is_integer_pgtype(t):
        cleaned = series.apply(_coerce_integer)
        all_ok = cleaned.isna().sum() == series.isna().sum()
        return cleaned, all_ok

    if _is_numeric_pgtype(t):
        cleaned = series.apply(_coerce_numeric)
        all_ok = cleaned.isna().sum() == series.isna().sum()
        return cleaned, all_ok

    cleaned = series.where(series.isna(), series.astype(str).str.strip())
    return cleaned, True


def _finalize_types(
        df: pd.DataFrame,
        headers: List[str],
        fields: List[FieldInfo]
) -> Tuple[pd.DataFrame, List[FieldInfo], Dict[str, str]]:
    """
    æ ¹æ® LLM å»ºè®®ç±»å‹é€åˆ—æ¸…æ´—ï¼›è‹¥å¤±è´¥åˆ™è‡ªåŠ¨é™çº§ï¼š
    - timestamp/date/time è§£æå¤±è´¥ â†’ text
    - integer/numeric å‡ºç°ä¸å¯è§£æ â†’ text
    - varchar åŠ¨æ€æ‰©å®¹ï¼ˆæˆ–è½¬ textï¼‰
    å¹¶å¯¹ç±»å‹åšç™½åå•é™å¹…ã€‚
    è¿”å›ï¼šæ¸…æ´—åçš„ dfã€æœ€ç»ˆå­—æ®µå®šä¹‰ã€æ¯åˆ—æœ€ç»ˆ PG ç±»å‹æ˜ å°„
    """
    final_df = df.copy()
    final_fields: List[FieldInfo] = []
    final_types: Dict[str, str] = {}

    # âœ… å…³é”®ä¿®å¤ï¼šæŒ‰â€œåˆ—ä½ç½®â€å¤„ç†ï¼Œè§„é¿é‡å¤è¡¨å¤´å¸¦æ¥çš„å®šä½ä¸ rename é—®é¢˜
    for idx, f in enumerate(fields):
        series = final_df.iloc[:, idx] if idx < final_df.shape[1] else pd.Series([], dtype=object)
        col_name = _sanitize_identifier(f.column)  # å†ä¿é™©
        suggested_pg_type = _sanitize_pg_type(f.type, series)

        cleaned, ok = _coerce_series_by_type(series, suggested_pg_type)
        eff_type = suggested_pg_type if ok else "text"
        if not ok:
            cleaned = series.astype(str).where(series.isna(), series.astype(str).str.strip())

        # æ–‡æœ¬ç±»å‹åŠ¨æ€è°ƒæ•´
        if (not _is_date_pgtype(eff_type)) and (not _is_timestamp_pgtype(eff_type)) and (not _is_time_pgtype(eff_type)) \
                and (not _is_boolean_pgtype(eff_type)) and (not _is_integer_pgtype(eff_type)) and (
                not _is_numeric_pgtype(eff_type)):
            eff_type = _adjust_text_type_by_length(cleaned)

        # å†™å›åŒä¸€ä½ç½®çš„åˆ—
        if idx < final_df.shape[1]:
            final_df.iloc[:, idx] = cleaned

        final_types[col_name] = eff_type
        final_fields.append(
            FieldInfo(column=col_name, name_cn=f.name_cn, type=eff_type, comment=f.comment)
        )

    return final_df, final_fields, final_types


# =========================
# å¤±è´¥å›é€€ï¼šçº¯æ–‡æœ¬å»ºè¡¨ & å…¨é‡å¯¼å…¥
# =========================
def _fallback_text_import(df: pd.DataFrame, table_name: str, engine, headers: List[str]) -> Dict[str, Any]:
    """
    ä»»æ„æ•°æ®å¿…è¾¾ç­–ç•¥ï¼šæŠŠæ‰€æœ‰åˆ—ä½œä¸º text å­˜å‚¨ï¼Œåˆ—åæŒ‰ header æ¸…æ´—å»é‡ï¼Œåˆ—æ³¨é‡Šä¿ç•™åŸå§‹ headerã€‚
    """
    SQLBotLogUtil.warning(f"ğŸ›Ÿ [AIå»ºè¡¨] è¿›å…¥é™çº§ï¼šæŒ‰ TEXT å…¨é‡å¯¼å…¥ â†’ è¡¨ {table_name}")
    safe_cols = _ensure_unique_columns(headers)
    col_defs = [f'"{c}" text' for c in safe_cols]

    # å…ˆæ„é€ åˆ—æ³¨é‡Š SQLï¼ˆé¿å…åœ¨ f-string è¡¨è¾¾å¼æ”¾å¸¦å¼•å·æ–‡å­—ï¼‰
    col_comments_lines = []
    for c, h in zip(safe_cols, headers):
        cmt = _sql_literal(str(h))
        col_comments_lines.append(f"COMMENT ON COLUMN \"{table_name}\".\"{c}\" IS '{cmt}';")

    _joiner = ",\n    "
    _cols_sql = _joiner.join(col_defs)
    _col_comments_sql = "\n".join(col_comments_lines)

    create_sql = (
        f'DROP TABLE IF EXISTS "{table_name}";\n'
        f'CREATE TABLE "{table_name}" (\n    {_cols_sql}\n);\n'
        f"{_col_comments_sql}"
    )
    SQLBotLogUtil.info(f"ğŸ§± [é™çº§å»ºè¡¨] SQL:\n{create_sql}")
    with engine.connect() as conn:
        conn.execute(text(create_sql))
        conn.commit()

    # âœ… åŒæ ·ç”¨æ•´ä½“æ›¿æ¢åˆ—åï¼Œè§„é¿é‡å¤è¡¨å¤´
    to_write = df.copy()
    to_write.columns = safe_cols
    to_write.astype(str).to_sql(
        table_name, engine,
        if_exists="append",
        index=False,
        chunksize=1000,
        method="multi"
    )
    SQLBotLogUtil.info(f"âœ… [é™çº§å»ºè¡¨] å·²ä»¥ TEXT æ¨¡å¼å¯¼å…¥ {len(df)} è¡Œ â†’ {table_name}")
    return {
        "table": table_name,
        "columns": [{"column": c, "name_cn": h, "type": "text", "comment": str(h)} for c, h in zip(safe_cols, headers)],
        "rows_inserted": len(df),
        "status": "fallback_text_success"
    }


# =========================
# ä¸»æµç¨‹ï¼ˆå¢å¼ºæ—¥å¿—+é™çº§ï¼Œå…¨é¢å…¼å®¹ 3.11ï¼‰
# =========================
def insert_pg_by_ai(df: pd.DataFrame, table_name: str, engine, sample_size: int = 10):
    """
    æ™ºèƒ½å»ºè¡¨ & å¯¼å…¥ï¼ˆLLM schema + å¼ºåˆ¶ç±»å‹æ¸…æ´— + å¤±è´¥è‡ªåŠ¨é™çº§ï¼‰
    ç›®æ ‡ï¼šåœ¨ä¸ç‰ºç‰²æ•°æ®å¯è½åº“æ€§çš„å‰æä¸‹æœ€å¤§åŒ–è¯­ä¹‰ç»“æ„åŒ–ç¨‹åº¦ï¼›è‹¥å¤±è´¥ï¼Œå¿…è¾¾é™çº§ã€‚
    """
    headers = list(df.columns)
    try:
        # 1) æ ·æœ¬
        try:
            sample_df = df.head(sample_size).convert_dtypes().copy()
            for col in sample_df.columns:
                if pd.api.types.is_datetime64_any_dtype(sample_df[col]):
                    sample_df[col] = sample_df[col].astype(str)
            sample_preview = json.dumps(sample_df.to_dict(orient="records"), ensure_ascii=False)
            SQLBotLogUtil.info(f"ğŸ§© [AIå»ºè¡¨] è¡¨ {table_name} æ ·æœ¬æ•°æ®é¢„è§ˆ: {sample_preview[:1200]}...")
        except:
            SQLBotLogUtil.warning(f"ğŸ§© [AIå»ºè¡¨] è¡¨ {table_name} æ ·æœ¬æ•°æ®é¢„è§ˆå¤±è´¥")
            sample_preview = []
        # 2) è°ƒç”¨ LLM
        loop = ensure_event_loop()
        llm = loop.run_until_complete(LLMManager.get_default_llm())
        parser = PydanticOutputParser(pydantic_object=TableSchema)

        prompt = ChatPromptTemplate.from_template("""
            ä½ æ˜¯ä¸€åèµ„æ·±æ•°æ®åº“å»ºæ¨¡ä¸“å®¶ï¼Œä¸“é•¿ PostgreSQLã€‚
            è¯·åŸºäºä»¥ä¸‹ Excel ä¿¡æ¯ï¼Œä¸ºå…¶ç”Ÿæˆ PG è¡¨ç»“æ„å®šä¹‰ã€‚

            è¾“å…¥ï¼š
            - è¡¨å¤´ï¼ˆæŒ‰é¡ºåºï¼‰ï¼š{headers}
            - æ ·æœ¬æ•°æ®ï¼ˆå‰ {sample_size} æ¡ï¼‰ï¼š{sample_preview}

            è¾“å‡ºï¼ˆä¸¥æ ¼ JSONï¼Œä»…æ­¤å†…å®¹ï¼‰ï¼š
            {format_instructions}

            è§„åˆ™ï¼š
            1) fields æ•°ç»„é•¿åº¦ä¸ headers å®Œå…¨ä¸€è‡´ã€é¡ºåºä¸€è‡´ã€‚
            2) columnï¼šè‹±æ–‡å°å†™ï¼Œä¸‹åˆ’çº¿ï¼›è¯­ä¹‰æ¸…æ™°ã€‚
            3) typeï¼šå¸¸ç”¨ PG ç±»å‹text/integer/bigint/numeric(10,2)/timestamp/date/booleanï¼‰ã€‚
            4) commentï¼šä½¿ç”¨headersæä¾›çš„æ˜¯ä¸­æ–‡åˆ™ä½¿ç”¨åŸæ–‡ã€‚å¦‚æœä¸æ˜¯ä¸­æ–‡åˆ™è‡ªå®šä¹‰ï¼ˆç®€æ´çš„è¯­ä¹‰æè¿°ï¼‰ã€‚
            5) ç”Ÿæˆ table_commentï¼ˆå¦‚â€œè®¾å¤‡é‡‡é›†è®°å½•è¡¨â€ï¼‰ã€‚
        """)

        chain = prompt | llm | parser
        llm_input = {
            "headers": headers,
            "sample_preview": sample_preview,
            "sample_size": sample_size,
            "format_instructions": parser.get_format_instructions()
        }

        SQLBotLogUtil.info("ğŸ§  [AIå»ºè¡¨] Prompt è¾“å…¥å†…å®¹:")
        SQLBotLogUtil.info(json.dumps(llm_input, ensure_ascii=False, indent=2)[:1500] + "...")

        result: TableSchema = loop.run_until_complete(
            asyncio.wait_for(chain.ainvoke(llm_input), timeout=60)
        )

        SQLBotLogUtil.info("ğŸ¤– [AIå»ºè¡¨] LLM è¾“å‡ºç»“æœ:")
        SQLBotLogUtil.info(json.dumps(result.model_dump(), ensure_ascii=False, indent=2))

        # 3) æ ¡éªŒä¸åˆ—åå”¯ä¸€åŒ–
        fields = result.fields
        if len(fields) != len(headers):
            raise HTTPException(status_code=500, detail=f"LLM è¿”å›å­—æ®µæ•°({len(fields)})ä¸è¡¨å¤´æ•°({len(headers)})ä¸ä¸€è‡´ã€‚")

        unique_cols = _ensure_unique_columns([f.column for f in fields])
        for i, f in enumerate(fields):
            fields[i] = FieldInfo(
                column=unique_cols[i],
                name_cn=f.name_cn,
                type=_sanitize_pg_type(f.type),  # å…ˆåšä¸€è½®ç±»å‹é™å¹…
                comment=f.comment
            )

        # 4) ç±»å‹æ¸…æ´—ä¸é™çº§ï¼ˆæŒ‰åˆ—ä½ç½®å¤„ç†ï¼Œå…¼å®¹é‡å¤è¡¨å¤´ï¼‰
        df_clean, fields_final, type_map = _finalize_types(df, headers, fields)

        # 5) ç”Ÿæˆ SQL â€”â€” å…ˆæ‹¼å—å†æ”¾å…¥ f-stringï¼ˆé¿å…è¡¨è¾¾å¼é‡ŒåŒ…å«åæ–œæ /å¼•å·å­—é¢é‡ï¼‰
        table_comment_esc = _sql_literal(result.table_comment or f"{table_name} æ•°æ®è¡¨")
        col_defs = [f'"{f.column}" {type_map[f.column]}' for f in fields_final]

        col_comments_lines = []
        for f in fields_final:
            cmt = _sql_literal(f.comment or "")
            col_comments_lines.append(f"COMMENT ON COLUMN \"{table_name}\".\"{f.column}\" IS '{cmt}';")

        _joiner = ",\n    "
        _cols_sql = _joiner.join(col_defs)
        _col_comments_sql = "\n".join(col_comments_lines)

        create_sql = (
            f'DROP TABLE IF EXISTS "{table_name}";\n'
            f'CREATE TABLE "{table_name}" (\n    {_cols_sql}\n);\n'
            f"COMMENT ON TABLE \"{table_name}\" IS '{table_comment_esc}';\n"
            f"{_col_comments_sql}"
        )
        SQLBotLogUtil.info(f"ğŸ§± [AIå»ºè¡¨] ç”Ÿæˆ SQL:\n{create_sql}")

        # 6) æ‰§è¡Œå»ºè¡¨ + å†™å…¥æ•°æ®ï¼ˆå¤±è´¥å³é™çº§ï¼‰
        try:
            with engine.connect() as conn:
                conn.execute(text(create_sql))
                conn.commit()

            # âœ… ç”¨æœ€ç»ˆåˆ—åæ•´ä½“è¦†ç›–ï¼Œé¿å…é‡å¤è¡¨å¤´ rename ä¸¢åˆ—
            to_write = df_clean.copy()
            to_write.columns = [f.column for f in fields_final]

            to_write.to_sql(
                table_name, engine,
                if_exists="append",
                index=False,
                chunksize=1000,
                method="multi"
            )
            SQLBotLogUtil.info(f"âœ… [AIå»ºè¡¨] æˆåŠŸå¯¼å…¥ {len(to_write)} è¡Œæ•°æ® â†’ {table_name}")

            return {
                "table": table_name,
                "columns": [f.model_dump() for f in fields_final],
                "rows_inserted": len(to_write),
                "sample_size": sample_size,
                "status": "success"
            }

        except Exception as write_err:
            SQLBotLogUtil.error(f"âŒ [AIå»ºè¡¨] å»ºè¡¨æˆ–å†™å…¥å¤±è´¥ï¼Œå°†è½¬å…¥ TEXT é™çº§ã€‚\n{write_err}\n{traceback.format_exc()}")
            # é™çº§ï¼šåˆ è¡¨åè½¬ TEXT
            with engine.connect() as conn:
                conn.execute(text(f'DROP TABLE IF EXISTS "{table_name}"'))
                conn.commit()
            return _fallback_text_import(df, table_name, engine, headers)

    except asyncio.TimeoutError:
        # LLM è¶…æ—¶ â†’ ç›´æ¥é™çº§
        return _fallback_text_import(df, table_name, engine, headers)
    except HTTPException:
        # è¯­ä¹‰ç±»å¼‚å¸¸ï¼ˆä¾‹å¦‚å­—æ®µæ•°ä¸åŒ¹é…ï¼‰ â†’ ç›´æ¥é™çº§
        return _fallback_text_import(df, table_name, engine, headers)
    except Exception as e:
        SQLBotLogUtil.error(f"âŒ [AIå»ºè¡¨] å¼‚å¸¸ï¼Œå°†è½¬å…¥ TEXT é™çº§ã€‚\n{e}\n{traceback.format_exc()}")
        return _fallback_text_import(df, table_name, engine, headers)


# =========================
# Event Loop å·¥å…·
# =========================
def ensure_event_loop() -> asyncio.AbstractEventLoop:
    """
    é€šç”¨äº‹ä»¶å¾ªç¯è·å–å™¨ï¼Œå…¼å®¹ Python 3.11+
    - ä¸»çº¿ç¨‹ï¼šå¤ç”¨å·²æœ‰ loopï¼›
    - å­çº¿ç¨‹ï¼šè‡ªåŠ¨åˆ›å»ºå¹¶ç»‘å®šæ–° loopï¼›
    """
    try:
        return asyncio.get_running_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop
