import hashlib
import json
import os
import traceback
import uuid

import pandas as pd
from fastapi import HTTPException
from sqlalchemy import text
from sqlmodel import select

from apps.datasource.crud.datasource import create_ds
from apps.datasource.models.datasource import CreateDatasource, CoreTable
from apps.datasource.utils.utils import aes_decrypt
from apps.datasource.utils.utils import aes_encrypt
from apps.db.engine import get_engine_conn
from common.core.config import settings
from common.core.deps import SessionDep
from common.utils.utils import SQLBotLogUtil
from .openapi_excle_create_table import insert_pg_by_ai
from ...datasource.api.datasource import insert_pg
from ...datasource.crud.field import delete_field_by_ds_id
from ...datasource.crud.table import delete_table_by_ds_id
from ...datasource.models.datasource import CoreDatasource, DatasourceConf

path = settings.EXCEL_PATH


def delete_ds(session: SessionDep, id: int):
    """
    åˆ é™¤æ•°æ®æºï¼ˆå«è¡¨æ¸…ç†é€»è¾‘ï¼‰
    - å¯¹ Excel ç±»å‹ï¼šä¼šåˆ é™¤æ‰€æœ‰å®é™…åˆ›å»ºçš„è¡¨ã€‚
    - å¯¹å…¶ä»–ç±»å‹ï¼šä»…æ¸…ç†å…ƒæ•°æ®ã€‚
    """
    from common.utils.utils import SQLBotLogUtil

    try:
        # === Step 1: è·å–æ•°æ®æºè®°å½• ===
        term = session.exec(select(CoreDatasource).where(CoreDatasource.id == id)).first()
        if not term:
            raise HTTPException(status_code=404, detail=f"Datasource with ID {id} not found")

        SQLBotLogUtil.info(f"ğŸ§© å‡†å¤‡åˆ é™¤æ•°æ®æº ID={id}, åç§°={term.name}, ç±»å‹={term.type}")

        # === Step 2: åˆ é™¤ Excel ç±»å‹çš„ç‰©ç†è¡¨ ===
        if term.type == "excel":
            try:
                conf_raw = term.configuration or ""
                if not conf_raw.strip():
                    SQLBotLogUtil.warning(f"âš ï¸ æ•°æ®æº ID={id} æ— é…ç½®é¡¹ configurationï¼Œè·³è¿‡è¡¨åˆ é™¤ã€‚")
                else:
                    conf = DatasourceConf(**json.loads(aes_decrypt(conf_raw)))
                    engine = get_engine_conn()
                    with engine.begin() as conn:
                        for sheet in conf.sheets:
                            table_name = sheet.get("tableName")
                            if not table_name:
                                SQLBotLogUtil.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆ Sheet é¡¹: {sheet}")
                                continue
                            SQLBotLogUtil.info(f"ğŸ—‘ï¸ åˆ é™¤ Excel è¡¨ï¼š{table_name}")
                            conn.execute(text(f'DROP TABLE IF EXISTS "{table_name}"'))
                    SQLBotLogUtil.info(f"âœ… å·²åˆ é™¤ Excel ç±»å‹æ•°æ®æºçš„å…¨éƒ¨è¡¨ã€‚")
            except Exception as e:
                SQLBotLogUtil.error(f"âŒ åˆ é™¤ Excel è¡¨å¤±è´¥: {traceback.format_exc()}")
                raise HTTPException(status_code=500, detail=f"Failed to drop Excel tables: {e}")

        # === Step 3: åˆ é™¤å…ƒæ•°æ® ===
        delete_table_by_ds_id(session, id)
        delete_field_by_ds_id(session, id)

        # === Step 4: åˆ é™¤æ•°æ®æºæœ¬èº« ===
        session.delete(term)
        session.commit()
        SQLBotLogUtil.info(f"âœ… æ•°æ®æºåˆ é™¤æˆåŠŸ ID={id}, åç§°={term.name}")

        return {"message": f"Datasource '{term.name}' (ID={id}) deleted successfully."}

    except HTTPException:
        # å·²çŸ¥é”™è¯¯ç›´æ¥æŠ›å‡º
        raise
    except Exception as e:
        session.rollback()
        SQLBotLogUtil.error(f"âŒ åˆ é™¤æ•°æ®æºå¼‚å¸¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Delete datasource failed: {e}")
    finally:
        # æ¸…ç†äº‹åŠ¡çŠ¶æ€
        if session.is_active:
            session.commit()


def insert_pg(df: pd.DataFrame, table_name: str, engine):
    """
    å°† DataFrame å†™å…¥ PostgreSQLï¼ˆreplace æ¨¡å¼ï¼‰
    - è¡¨åï¼šå¤–å±‚ä¼ å…¥ï¼ˆf1, f2, ...ï¼‰
    - å­—æ®µåï¼šè‡ªåŠ¨ç”Ÿæˆä¸º c1, c2, ...
    - å­—æ®µæ³¨é‡Šï¼šä¿ç•™ Excel åŸå§‹ header åç§°
    """
    try:
        # === 1ï¸âƒ£ é‡å‘½å DataFrame åˆ— ===
        original_columns = list(df.columns)
        new_columns = [f"h{i + 1}" for i in range(len(original_columns))]
        rename_map = dict(zip(original_columns, new_columns))
        df = df.rename(columns=rename_map)

        # === 2ï¸âƒ£ å†™å…¥ PostgreSQL ===
        df.to_sql(table_name, engine, if_exists="replace", index=False)

        # === 3ï¸âƒ£ å†™å…¥å­—æ®µæ³¨é‡Š ===
        with engine.connect() as conn:
            for new_col, orig_col in rename_map.items():
                comment_sql = text(f'COMMENT ON COLUMN "{table_name}"."{orig_col}" IS :comment')
                conn.execute(comment_sql, {"comment": new_col})
            conn.commit()

    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to insert table {table_name}: {e}")


def upload_excel_and_create_datasource_service(session, trans, user, save_path: str, original_filename: str,
                                                     example_size: int = 10,
                                                     ai: bool = False, ):
    """
    ä¸Šä¼  Excel å¹¶è‡ªåŠ¨åˆ›å»ºæ•°æ®æºï¼ˆExcelç±»å‹ï¼‰
    ç”¨äº openapi å±‚çš„å¤ç”¨ã€‚
    - è‡ªåŠ¨è·³è¿‡ç©º sheet
    - è‡ªåŠ¨å¤„ç†é‡åæ–‡ä»¶
    - è‡ªåŠ¨åŠ å¯† configuration
    - å¼‚å¸¸æ—¶æ¸…ç†å­¤è¡¨
    - å…¼å®¹ .csv / .xlsx / .xls / .ods / .xlsb ç­‰æ ¼å¼
    - å…¼å®¹ Pydantic v2 (from_attributes)
    """
    created_tables = []  # æˆåŠŸå¯¼å…¥çš„è¡¨å
    try:
        SQLBotLogUtil.info(f"ğŸ“‚ å¼€å§‹ä¸Šä¼ å¹¶åˆ›å»ºæ•°æ®æºï¼š{original_filename}")
        engine = get_engine_conn()

        # === Step 0: åŸºæœ¬æ ¡éªŒ ===
        if not os.path.exists(save_path) or os.path.getsize(save_path) < 10:
            raise HTTPException(status_code=400, detail="Uploaded file is empty or invalid.")

        # === Step 1: è§£æ Excel/CSV ===
        file_ext = os.path.splitext(original_filename)[1].lower()

        if file_ext == ".csv":
            try:
                df = pd.read_csv(save_path, engine="c")
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"CSV parsing failed: {e}")

            if df.empty or len(df.columns) == 0 or df.dropna(how="all").empty:
                SQLBotLogUtil.info("âš ï¸ è·³è¿‡ CSVï¼šæ— æœ‰æ•ˆæ•°æ®")
            else:
                table_name = f"sheet1_{hashlib.sha256(uuid.uuid4().bytes).hexdigest()[:10]}"
                if ai is True:
                    insert_pg_by_ai(df, table_name, engine, example_size)
                else:
                    insert_pg(df, table_name, engine)
                created_tables.append(table_name)
                SQLBotLogUtil.info(f"âœ… å¯¼å…¥ CSV å®Œæˆï¼š{table_name}")

        else:
            sheet_names = pd.ExcelFile(save_path).sheet_names
            for sheet_name in sheet_names:
                try:
                    df = pd.read_excel(save_path, sheet_name=sheet_name, engine='calamine')
                except Exception as e:
                    SQLBotLogUtil.error(f"âš ï¸ è¯»å– Sheet [{sheet_name}] å¤±è´¥: {e}")
                    continue

                if df.empty or len(df.columns) == 0 or df.dropna(how="all").empty:
                    SQLBotLogUtil.info(f"âš ï¸ è·³è¿‡ç©º Sheetï¼š{sheet_name}")
                    continue

                table_name = f"{sheet_name}_{hashlib.sha256(uuid.uuid4().bytes).hexdigest()[:10]}"
                if ai is True:
                    insert_pg_by_ai(df, table_name, engine, example_size)
                else:
                    insert_pg(df, table_name, engine)
                created_tables.append(table_name)
                SQLBotLogUtil.info(f"âœ… å¯¼å…¥ Sheet å®Œæˆï¼š{table_name}")

        if not created_tables:
            SQLBotLogUtil.error("âŒ Excel å†…æ— æœ‰æ•ˆæ•°æ®è¡¨ï¼Œç»ˆæ­¢åˆ›å»ºã€‚")
            raise HTTPException(status_code=400, detail="Excel has no valid data sheets")

        # === Step 2: æ„å»ºé…ç½®å¹¶åŠ å¯† ===
        conf_dict = {
            "file_path": save_path,
            "sheets": [{"tableName": tname, "tableComment": ""} for tname in created_tables],
        }
        configuration_encrypted = aes_encrypt(json.dumps(conf_dict, ensure_ascii=False))

        # === Step 3: ç”Ÿæˆå”¯ä¸€åç§° ===
        suffix = uuid.uuid4().hex[:6]
        ds_name = f"{original_filename.split('.')[0]}_{suffix}.{original_filename.split('.')[-1]}"

        # === Step 4: åˆ›å»ºæ•°æ®æºå¯¹è±¡ ===
        tables_payload = [CoreTable(table_name=tname, table_comment="") for tname in created_tables]
        ds = CreateDatasource(
            name=ds_name,
            type="excel",
            configuration=configuration_encrypted,
            tables=tables_payload,
        )

        datasource = create_ds(session, trans, user, ds)
        session.flush()
        SQLBotLogUtil.info(f"âœ… æ•°æ®æºåˆ›å»ºæˆåŠŸ ID={datasource.id}")
        return datasource

    except Exception as e:
        SQLBotLogUtil.error(f"âŒ ä¸Šä¼ å¹¶åˆ›å»ºæ•°æ®æºå¤±è´¥: {traceback.format_exc()}")
        # === Step 6: æ¸…ç†å­¤è¡¨ ===
        if created_tables:
            SQLBotLogUtil.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†å­¤è¡¨ï¼Œå…± {len(created_tables)} ä¸ª")
            try:
                engine = get_engine_conn()
                with engine.connect() as conn:
                    for tname in created_tables:
                        conn.execute(text(f'DROP TABLE IF EXISTS \"{tname}\"'))
                        SQLBotLogUtil.info(f"ğŸ§¹ å·²æ¸…ç†å­¤è¡¨ï¼š{tname}")
                    conn.commit()
            except Exception as drop_err:
                SQLBotLogUtil.error(f"âš ï¸ æ¸…ç†å­¤è¡¨å¤±è´¥: {drop_err}")

        raise HTTPException(status_code=500, detail=f"Upload and create datasource failed: {e}")
